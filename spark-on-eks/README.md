# SQL-based ETL with Spark on EKS
A project for a solution - SQL based ETL with a declarative framework powered by Apache Spark. 

## Spark on EKS Overview
![](images/architecture.png)

### Submit ETL job in k8s
![](images/submit_native_spark.gif)

#### Table of Contents
* [Prerequisites](#Prerequisites) 
* [Deploy CloudFormation](#Launch-the-CFN)
* [Post Deployment](#Post-Deployment)
  * [Run a script](#run-a-script)
  * [Test ETL job in Arc Jupyter](#test-etl-job-in-Jupyter)
  * [Submit job on Argo UI](#Submit-etl-job-on-Argo-UI)
  * [Submit job via Argo CLI](#Submit-etl-job-via-Argo-CLI)
  * [Submit vanilla Spark job](#Submit-a-vanilla-Spark-job-with-Spark-Operator)
    * [Execute a PySpark job](#Execute-a-PySpark-job)
    * [Self-recovery test](#Self-recovery-test)
    * [Cost savings with spot instance](#Check-Spot-instance-usage-and-cost-savings)
    * [Auto scaling & Dynamic resource allocation](#Autoscaling---dynamic-allocation-support)
* [Useful Commands](#Useful-Commands)  
* [Clean Up](#clean-up)

## Prerequisites 

1. AWS CLI is configured to communicate with services in your deployment account. Either set your profile by `export AWS_PROFILE=<your_aws_profile>` , or run `aws configure`.
2. [AWS CloudShell](https://console.aws.amazon.com/cloudshell/) is available in your deployment **region**. Otherwise, run all post deployment commands in a local computer.

## Download the project

```bash
git clone https://github.com/aws-samples/sql-based-etl-on-amazon-eks.git

```

## Launch the CFN

The provisining takes about 30 minutes to complete. 

  |   Region  |   Launch Template |
  |  ---------------------------   |   -----------------------  |
  |  ---------------------------   |   -----------------------  |
  **US East (N. Virginia)**| [![Deploy to AWS](images/00-deploy-to-aws.png)](https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/quickcreate?stackName=SparkOnEKS&templateURL=https://blogpost-sparkoneks-us-east-1.s3.amazonaws.com/blog/v1.0.0/SparkOnEKS.template) 

* Option1: Deploy with default (recommended). Use the following script to deploy in a different region. 
* Option2: Fill in the parameter `jhubuser` if you want to setup a customized username for Jupyter login. 
* Option3: If ETL your own data, input the parameter `datalakebucket` with your S3 bucket. 
`NOTE: the S3 bucket must be in the same region as the deployment region.`

## Customization

Build your own solution on top of the project, for example reconfigure the Jupyter notebook, then generate the CFN in your region: 

```bash
# go to the project directory
cd spark-on-eks

export BUCKET_NAME_PREFIX=<your_bucket_name> # bucket where customized code will reside
export AWS_REGION=<your_region>
export SOLUTION_NAME=blog
export VERSION=v1.0.0 # version number for the customized code

./deployment/build-s3-dist.sh $BUCKET_NAME_PREFIX $SOLUTION_NAME $VERSION

# create the bucket where customized code will reside
aws s3 mb s3://$BUCKET_NAME_PREFIX-$AWS_REGION --region $AWS_REGION

# Upload deployment assets to the S3 bucket
aws s3 cp ./deployment/global-s3-assets/ s3://$BUCKET_NAME_PREFIX-$AWS_REGION/$SOLUTION_NAME/$VERSION/ --recursive --acl bucket-owner-full-control
aws s3 cp ./deployment/regional-s3-assets/ s3://$BUCKET_NAME_PREFIX-$AWS_REGION/$SOLUTION_NAME/$VERSION/ --recursive --acl bucket-owner-full-control

echo -e "\nIn web browser, paste the URL to launch the template: https://console.aws.amazon.com/cloudformation/home?region=$AWS_REGION#/stacks/quickcreate?stackName=SparkOnEKS&templateURL=https://$BUCKET_NAME_PREFIX-$AWS_REGION.s3.amazonaws.com/$SOLUTION_NAME/$VERSION/SparkOnEKS.template"
```

[*^ back to top*](#Table-of-Contents)
## Post-deployment

### Run a script

Go to AWS CloudShell:[[link to AWS CloudShell]](https://console.aws.amazon.com/cloudshell/), select your **region** the solution was deployed. Run the command:

 ```bash
 curl https://raw.githubusercontent.com/aws-samples/sql-based-etl-on-amazon-eks/main/spark-on-eks/deployment/post-deployment.sh | bash
 ```
 Or
run it on your computer:
```bash
./deployment/post-deployment.sh 'run-local'
```

[*^ back to top*](#Table-of-Contents)
### Test ETL job in Jupyter
1. Login with the details provided by the script above.

NOTE: The notebook session refreshes every 30 minutes. You may lose your work if it hasn't saved on time. The notebook allows you to download, and the capability is configurable, ie. you can disable it in order to improve your data security.

2. Open a sample job `sql-based-etl/source/example/notebook/scd2-job.ipynb` on your Jupyter notebook instance. The job outputs a table to support the [Slowly Changing Dimension Type 2](https://www.datawarehouse4u.info/SCD-Slowly-Changing-Dimensions.html) business need.

3. [FYI] The source [contacts data](/deployment/app_code/data/) was generated by a [python script](https://raw.githubusercontent.com/cartershanklin/hive-scd-examples/master/merge_data/generate.py)

![](source/images/fake_data.gif)

4. Execute each block and observe the result. Change embedded SQL scripts if you like. By walking through each step in the notebook, you will get a hands-on experience on how the SQL-based ETL job works powered by Apache SparkSQL.

5. [FYI] To demonstrate the best practice in DataDevOps, the JupyterHub is configured to synchronize the latest code from the github repo each time when you login. In real practice, you must check-in all changes to a source repository, in order to save and trigger your ETL pipelines.

6. Run a query in [Athena console](https://console.aws.amazon.com/athena/) to see if it is a SCD2 type table. 
```bash
SELECT * FROM default.deltalake_contact_jhub WHERE id=12
```

[*^ back to top*](#Table-of-Contents)
### Submit ETL job on Argo UI

Check your connection in [AWS CloudShell](https://console.aws.amazon.com/cloudshell/) or local computer. If no access, go to [Run a post-deployment script](#run-a-script).
```bash
# check EKS connection
kubectl get svc
```
1. Go to the Argo workflow console found in the [CloudFormation outputs](https://console.aws.amazon.com/cloudformation/) under a stack named `SparkOnEKS`. 
2. Run `argo auth token` command, then copy & paste the token to your Argo website.
3. Click `SUBMIT NEW WORKFLOW` button, replace content by the followings, then `SUBMIT`. Click a pod (dot) to check the job status and application logs.

  ```yaml
  apiVersion: argoproj.io/v1alpha1
  kind: Workflow
  metadata:
    generateName: nyctaxi-job-
    namespace: spark
  spec:
    serviceAccountName: arcjob
    entrypoint: nyctaxi
    templates:
    - name: nyctaxi
      dag:
        tasks:
          - name: step1-query
            templateRef:
              name: spark-template
              template: sparkLocal
              clusterScope: true   
            arguments:
              parameters:
              - name: jobId
                value: nyctaxi  
              - name: tags
                value: "project=sqlbasedetl, owner=myowner, costcenter=66666"  
              - name: configUri
                value: https://raw.githubusercontent.com/tripl-ai/arc-starter/master/examples/kubernetes/nyctaxi.ipynb
              - name: parameters
                value: "--ETL_CONF_DATA_URL=s3a://nyc-tlc/trip*data \
                --ETL_CONF_JOB_URL=https://raw.githubusercontent.com/tripl-ai/arc-starter/master/examples/kubernetes"
  ```
  ![](images/3-argo-job-dependency.png)

### Submit ETL job via Argo CLI

Now, let's submit the SCD2 notebook tested in Jupyter via a commandline tool. To mock up a real-world scenario, we have broken it down to 3 files, ie. ETL jobs, stored in "[spark-on-eks/deployment/app_code/job/]"(/deployment/app_code/job). 

1. Submit the job and check the progress in Argo web console.
```bash
app_code_bucket=$(aws cloudformation describe-stacks --stack-name SparkOnEKS --query "Stacks[0].Outputs[?OutputKey=='CODEBUCKET'].OutputValue" --output text)
argo submit https://raw.githubusercontent.com/aws-samples/sql-based-etl-on-amazon-eks/main/spark-on-eks/source/example/scd2-job-scheduler.yaml -n spark --watch  -p codeBucket=$app_code_bucket

# or submit the job menifest file from local computer
argo submit source/example/scd2-job-scheduler.yaml -n spark --watch  -p codeBucket=$app_code_bucket
```

2. Query the table in [Athena](https://console.aws.amazon.com/athena/) to see if it has the same outcome as the test in Jupyter earlier. 

```sql
SELECT * FROM default.contact_snapshot WHERE id=12
``` 

[*^ back to top*](#Table-of-Contents)
### Submit a vanilla Spark job with Spark Operator

Previously, we have run the CloudFormation-like ETL job defined in Jupyter notebook. They are powered by the [Arc data framework](https://arc.tripl.ai/). It dramatically simplifies and accerlerates the data application development with zero line of code. In this exmaple, we will reuse the Arc docker image, because it contains the latest Spark distribution. Let's run a vanilla Spark job that is defined by k8s's CRD [Spark Operator](https://operatorhub.io/operator/spark-gcp). It saves efforts on DevOps operation, as the way of deploying Spark application follows the same declarative approach in k8s. It is consistent with other business applications CICD deployment processes.
  The example demonstrates:
  * Save cost with [Amazon EC2 Spot instance](https://aws.amazon.com/ec2/spot/) type
  * Dynamically scale a Spark application - via [Dynamic Resource Allocation](https://spark.apache.org/docs/3.0.0-preview/job-scheduling.html#dynamic-resource-allocation)
  * Self-recovery after losing a Spark driver
  * Monitor a job on Spark WebUI

[*^ back to top*](#Table-of-Contents)
#### Execute a PySpark job

Submit a Spark job to EKS as usual. The pySpark application is stored `spark-on-eks/deployment/app_code/job/wordcount.py`(/deployment/app_code/job/wordcount.py)

```bash
app_code_bucket=$(aws cloudformation describe-stacks --stack-name SparkOnEKS --query "Stacks[0].Outputs[?OutputKey=='CODEBUCKET'].OutputValue" --output text)
kubectl create -n spark configmap special-config --from-literal=codeBucket=$app_code_bucket
kubectl apply -f https://raw.githubusercontent.com/aws-samples/sql-based-etl-on-amazon-eks/main/spark-on-eks/source/example/native-spark-job-scheduler.yaml
```

```bash
# If you connected the EKS from your computer, submit the job locally
kubectl apply -f source/example/native-spark-job-scheduler.yaml

# watch the progress in EKS
kubectl get pod -n spark

# watch job progress on SparkUI
kubectl port-forward word-count-driver 4040:4040 -n spark
# go to `localhost:4040` from your web browser
```

[*^ back to top*](#Table-of-Contents)
#### Self-recovery test
In Spark world, we know the driver is a single point of failure of a Spark application. If driver dies, all other linked components will be discarded too. Outside of Kubernetes, it requires extra effort to set up a job rerun, in order to provide the fault tolerance capability, however It is much simpler in Amazon EKS. 

The pySpark job takes approx. 10 minutes to finish. Let's test the self-recovery against the active Spark cluster.

* Driver test - manually kill the EC2 instance running your Spark driver:
```bash
Kubectl describe pod word-count-driver -n spark

# delete the EC2 server found in the description
kubectl delete node <ec2_host_name>
# has the driver come back?
kubectl get pod -n spark

```
See the demonstration below, which simulates the Spot interruption scenario: 
![](/spark-on-eks/images/driver_interruption_test.gif)

* Executor test - kill one of executors: 

```bash
# get an executor pod name
kubectl get pod -n spark
# replace the example pod name by yours
kubectl delete -n spark pod <example:amazon-reviews-word-count-51ac6d777f7cf184-exec-1> --force
# has it come back with a different number suffix? 
kubectl get pod -n spark
```

[*^ back to top*](#Table-of-Contents)
#### Check Spot instance usage and cost savings
Go to [Spot Request console](https://console.aws.amazon.com/ec2sp/v2/) -> Saving Summary, to find out how much running cost you just saved.

#### Autoscaling & Dynamic Allocation support

The job ends up with 20 Spark executors/pods on 7 spot EC2 instances, approx. 3 executors per EC2 spot instance. It takes 10 minutes to complete. 

Once the job starts, you will see your Spark cluster scales from 0 to 10 executors. Eventually, the Spark cluster lands with 20 executors, driven by the DynamicAllocation capability in Spark.

The auto-scaling is configured to be balanced across two AZs. Depending on your business requirement, you can fit the ETL job into a single AZ if needed.

```bash
kubectl get node --label-columns=lifecycle,topology.kubernetes.io/zone
kubectl get pod -n spark
```
![](images/4-auto-scaling.png)

[*^ back to top*](#Table-of-Contents)
## Useful Commands
 * `argo submit source/example/nyctaxi-job-scheduler.yaml`  submit a spark job via Argo
 * `argo list --all-namespaces`                       show all jobs scheduled via Argo
 * `kubectl get pod -n spark`                         list running Spark jobs
 * `kubectl delete pod --all -n spark`                delete all Spark jobs
 * `kubectl apply -f source/app_resources/spark-template.yaml` create a reusable Spark job template

[*^ back to top*](#Table-of-Contents)
## Clean up
Go to the repo's root directory, and run the clean-up script with your CloudFormation stack name.The default value is SparkOnEKS. If an error "(ResourceInUse) when calling the DeleteTargetGroup operation" occurs, simply run the script again.
```bash
cd sql-based-etl-on-amazon-eks/spark-on-eks
./deployment/delete_all.sh
```
Go to your [CloudFormation console](https://console.aws.amazon.com/cloudformation/home?region=us-east-1), manually delete the remaining resources if needed.
